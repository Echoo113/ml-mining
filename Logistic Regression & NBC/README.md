# ğŸš€ **Logistic Regression & Naive Bayes Classifier - HW2** ğŸ¯

## ğŸ“Œ **Overview**
This repository contains my implementation of a **Logistic Regression classifier** and a **Naive Bayes classifier** as part of my university coursework. The goal of this assignment is to implement these fundamental machine learning algorithms and apply them to a dataset.

## ğŸ“‚ **Files & Implementation**
### ğŸ”§ **Edited Files (To Be Submitted)**
- `lr.py` â€“ Implements the **Logistic Regression classifier**.
- `nbc.py` â€“ Implements the **Naive Bayes classifier**.

### ğŸ“ **Provided Files (No Edits Needed)**
- `utils.py` â€“ Contains helper functions for dataset loading and processing.
- `train.pkl` â€“ The dataset used for training the models.

---

## ğŸ“Š **Logistic Regression Implementation**
Logistic Regression is a supervised learning algorithm used for binary classification. It predicts probabilities using the **sigmoid function**:
  
$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

where \( z = Xw \) and \( w \) is the weight vector.

### ğŸ›  **Key Implementations**
1ï¸âƒ£ **Constructor** â€“ Initializes attributes `self.w`, `self.X`, and `self.y` as `None`.  
2ï¸âƒ£ **Sigmoid Function** â€“ Computes the probability for logistic regression using the formula above.  
3ï¸âƒ£ **Initialize Weights** â€“ Sets initial model weights as an array of ones.  
4ï¸âƒ£ **Compute Gradient** â€“ Uses the gradient of the loss function:  

$$
\nabla_w L(w) = \frac{1}{m} X^T (\sigma(Xw) - y)
$$

5ï¸âƒ£ **Fit Function** â€“ Trains the model using **Gradient Descent** with update rule:

$$
w_{i+1} = w_i - \alpha \nabla_w L(w_i)
$$

where \( \alpha \) is the **learning rate**.  
6ï¸âƒ£ **Predict Function** â€“ Predicts labels based on probabilities:

$$
\hat{y} =
\begin{cases} 
1, & P(y=1|x) = \sigma(Xw) \geq 0.5 \\
0, & \text{otherwise}
\end{cases}
$$

7ï¸âƒ£ **Accuracy Function** â€“ Computes model accuracy:

$$
\text{Accuracy} = \frac{\text{Number of correct predictions}}{\text{Total predictions}}
$$

âœ… **Expected Performance**: Training accuracy **â‰¥ 0.70**, Validation accuracy **â‰¥ 0.65**.

---

## ğŸ“Š **Naive Bayes Classifier Implementation**
Naive Bayes is a **probabilistic classifier** based on Bayes' Theorem:

$$
P(y|X) = \frac{P(X|y) P(y)}{P(X)}
$$

where:  
- \( P(y) \) is the **prior probability**  
- \( P(X|y) \) is the **likelihood**  
- \( P(X) \) is the **evidence**  
- \( P(y|X) \) is the **posterior probability**

### ğŸ›  **Key Implementations**
1ï¸âƒ£ **Constructor** â€“ Initializes attributes including `self.alpha` for Laplace smoothing.  
2ï¸âƒ£ **Prior Probability Computation** â€“ Uses:

$$
P(y=c) = \frac{\text{Number of samples with label } c}{\text{Total samples}}
$$

3ï¸âƒ£ **Feature Probability Computation** â€“ Uses Laplace smoothing:

$$
P(x_j | y=c) = \frac{\text{Count}(x_j, y=c) + \alpha}{\text{Total count for } y=c + \alpha d}
$$

where \( d \) is the number of possible feature values.  
4ï¸âƒ£ **Fit Function** â€“ Stores dataset and computes prior & feature probabilities.  
5ï¸âƒ£ **Predict Probabilities** â€“ Computes class probabilities for test data.  
6ï¸âƒ£ **Predict Function** â€“ Assigns class label with **highest probability**.  
7ï¸âƒ£ **Evaluation Function** â€“ Computes losses:

$$
\text{Zero-One Loss} = \frac{\text{Number of incorrect predictions}}{\text{Total predictions}}
$$

$$
\text{Squared Loss} = \frac{1}{m} \sum_{i=1}^{m} (1 - p_i)^2
$$

âœ… **Expected Performance**: Zero-one loss **â‰¤ 0.3**, Squared loss **â‰¤ 0.25**.

---

## ğŸ“¦ **Required Packages**
The following packages are required:
- âœ… `numpy` (Required)
- ğŸ†— `tqdm` (Optional, for progress bar)

To install:

```bash
pip install numpy tqdm
