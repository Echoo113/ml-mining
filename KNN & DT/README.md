# ğŸš€ **KNN & Decision Tree Classifier ** ğŸ¯

## ğŸ“Œ **Overview**
This repository contains my implementation of a **K-Nearest Neighbors (KNN) classifier** and a **Decision Tree classifier** as part of my university coursework. The goal of this assignment is to implement these fundamental machine learning algorithms and apply them to a dataset.

## ğŸ“‚ **Files & Implementation**
### ğŸ”§ **Edited Files (To Be Submitted)**
- `knn.py` â€“ Implements the **KNN classifier**.
- `scorer.py` â€“ Implements scoring functions for decision trees.
- `dt.py` â€“ Implements the **Decision Tree classifier**.

### ğŸ“ **Provided Files (No Edits Needed)**
- `utils.py` â€“ Contains helper functions.
- Other test files used for evaluation.

---

## ğŸ¤– **K-Nearest Neighbors (KNN) Implementation**
KNN is a non-parametric algorithm that classifies a sample based on the majority label among its `k` nearest neighbors. I implemented it using **Minkowski distance (p=3)**.

### ğŸ›  **Key Implementations**
1ï¸âƒ£ **Constructor** â€“ Initializes `self.k`, `self.X_train`, and `self.y_train`.
2ï¸âƒ£ **Fit Function** â€“ Stores training data for reference.
3ï¸âƒ£ **Minkowski Distance** â€“ Computes distance between test and training samples:

```math
D(x, y) = \left(\sum |x_i - y_i|^p\right)^{\frac{1}{p}}
```

4ï¸âƒ£ **Find Neighbors** â€“ Identifies `k` closest training samples.
5ï¸âƒ£ **Predict Function** â€“ Assigns the most common label among `k` neighbors.

âœ… **Expected Performance**: Accurate classification with increasing `k` improving stability.

---

## ğŸŒ³ **Decision Tree Implementation**
A Decision Tree splits data based on feature values to maximize information gain or minimize impurity.

### ğŸ›  **Key Implementations**
1ï¸âƒ£ **Scoring Functions** â€“ Computes **Information Gain**, **Gini Impurity**, and **Chi-square Gain** for best splits.

- **Entropy** (Information Score):
```math
H(S) = -\sum p_i \log_2 p_i
```

- **Gini Impurity**:
```math
G(S) = 1 - \sum p_i^2
```

- **Information Gain**:
```math
IG(S, A) = H(S) - \sum \frac{|S_v|}{|S|} H(S_v)
```

- **Chi-square Gain**:
```math
\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}
```

2ï¸âƒ£ **Subset Splitting** â€“ Divides data based on feature values.
3ï¸âƒ£ **Build Tree Function** â€“ Recursively splits dataset until stopping conditions are met.
4ï¸âƒ£ **Predict Function** â€“ Traverses the tree to classify new samples.

âœ… **Expected Performance**: Well-balanced trees that generalize to unseen data.

---

## ğŸ“¦ **Required Packages**
- âœ… `numpy`
- âœ… `pandas`
- âœ… `scipy` (for chi-square test)
- ğŸ†— `tqdm` (optional, for progress visualization)

To install:
```bash
pip install numpy pandas scipy tqdm
```

---

## ğŸ† **Evaluation & Submission**
- Run local tests:
  ```bash
  python <filename>.py
  ```
- Submit `knn.py`, `scorer.py`, `dt.py` to **Gradescope**.
- Pass public tests to validate implementation.
- Hidden test cases will determine final grade.

ğŸš€ **Letâ€™s classify some data!** ğŸ”¥
