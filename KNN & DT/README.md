# ğŸ§  KNN & Decision Tree Classifier Implementation

Welcome to my implementation of **K-Nearest Neighbors (KNN) and Decision Tree classifiers**! ğŸš€
This repository contains the solutions for HW1, where I implemented these fundamental machine learning algorithms from scratch using Python. ğŸğŸ’¡

## ğŸ“‚ Project Structure
Here's what you'll find in this repository:

```
ğŸ“¦ hw1
 â”£ ğŸ“œ knn.py        # KNN classifier implementation ğŸƒâ€â™‚ï¸
 â”£ ğŸ“œ scorer.py     # Scoring functions for decision trees ğŸ¯
 â”£ ğŸ“œ dt.py         # Decision tree classifier implementation ğŸŒ³
 â”£ ğŸ“œ utils.py      # Helper functions (no modifications needed) ğŸ”§
 â”— ğŸ“œ README.md     # You are here! ğŸ“–
```

## ğŸ“– Overview
This assignment focuses on implementing:
âœ… **K-Nearest Neighbors (KNN)** using Minkowski distance (p=3) ğŸ“
âœ… **Decision Tree Classifier** with various splitting criteria ğŸŒ³
âœ… **Scoring Functions** (Entropy, Gini, and Chi-square gain) ğŸ“Š

## ğŸš€ Implementation Details
### ğŸ”¹ KNN Classifier
- Implements a **non-parametric classifier** that predicts based on neighbors.
- Uses **Minkowski distance** (p=3) as the metric.
- Efficiently retrieves **top-K nearest neighbors** using `np.argsort`.
- Predicts labels using **majority voting** (`np.bincount` & `np.argmax`).

#### ğŸ“ Minkowski Distance Formula
The Minkowski distance of order \( p \) between two points \( x \) and \( y \) in an \( n \)-dimensional space is given by:
$$
D(x, y) = \left( \sum_{i=1}^{n} |x_i - y_i|^p \right)^{\frac{1}{p}}
$$
For this assignment, we use \( p = 3 \).

### ğŸ”¹ Scorer Functions
- **Entropy (Information Score)**: Measures uncertainty in class labels.

  $$
  H(S) = - \sum_{i=1}^{c} p_i \log_2 p_i
  $$
  where \( p_i \) is the probability of class \( i \).

- **Gini Score**: Measures impurity in a dataset.
  $$
  G(S) = 1 - \sum_{i=1}^{c} p_i^2
  $$

- **Chi-square Gain**: Uses contingency tables to evaluate feature splits.
  $$
  \chi^2 = \sum \frac{(O - E)^2}{E}
  $$
  where \( O \) is the observed frequency and \( E \) is the expected frequency.

- Implements **subset selection, information gain, and Gini gain calculations**.

### ğŸ”¹ Decision Tree Classifier
- Uses **recursive partitioning** to build a tree from training data.
- Splits nodes based on **maximum information gain**.
- Supports **class probability predictions** at leaf nodes.
- Implements **pruning using max depth** to prevent overfitting.

## ğŸ“¦ Dependencies
Before running the code, install the required libraries:
```bash
pip install numpy pandas scipy
```

## ğŸ¯ Usage
Run the classifiers using the provided datasets:
```python
# KNN Example ğŸƒâ€â™‚ï¸
k = KNearestNeighbor(k=5)
k.fit(X_train, y_train)
predictions = k.predict(X_test)
```

```python
# Decision Tree Example ğŸŒ³
dt = DecisionTree(max_depth=5)
dt.fit(X_train, y_train)
predictions = dt.predict(X_test)
```

## ğŸ† Why This Matters
These classifiers are foundational in machine learning and serve as building blocks for more complex models! Understanding their implementation helps in **optimizing real-world decision-making**. ğŸ§ ğŸ’¡

---
Made with â¤ï¸ by **[Your Name]** | ğŸš€ Happy Coding!
